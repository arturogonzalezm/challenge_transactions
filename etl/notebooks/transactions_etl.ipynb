{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RAW_FILE_PATH = '../../data/raw/Transaction.csv'\n",
    "OUTPUT_PATH = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../../schema_definition/schema.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from schema_definition.schema import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 22:04:44 WARN Utils: Your hostname, Arturos-Mac-mini.local resolves to a loopback address: 127.0.0.1; using 192.168.1.26 instead (on interface en1)\n",
      "21/10/15 22:04:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/arturogonzalez/Documents/dev/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/15 22:04:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.9 (default, Aug 21 2021 15:53:23)\n",
      "Spark context Web UI available at http://192.168.1.26:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1634295885029).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.shell import spark\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 22:04:54 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 19, schema size: 17\n",
      "CSV file: file:///Users/arturogonzalez/DataspellProjects/transactions_notebooks/data/raw/Transaction.csv\n"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read.csv(RAW_FILE_PATH, sep=',', header=True, schema=bronze_schema, enforceSchema=True)\n",
    "raw_df.repartition(1).write.csv(OUTPUT_PATH + 'bronze', header=True, sep='|', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 22:05:35 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 17, schema size: 19\n",
      "CSV file: file:///Users/arturogonzalez/DataspellProjects/transactions_notebooks/data/bronze/part-00000-e1272132-8e2d-4918-915d-cdf1595c2ce6-c000.csv\n",
      "21/10/15 22:05:36 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 17, schema size: 19\n",
      "CSV file: file:///Users/arturogonzalez/DataspellProjects/transactions_notebooks/data/bronze/part-00000-e1272132-8e2d-4918-915d-cdf1595c2ce6-c000.csv\n"
     ]
    }
   ],
   "source": [
    "bronze_df = spark.read.csv(OUTPUT_PATH + 'bronze/*.csv', sep='|', header=True, schema=silver_schema, enforceSchema=True)\n",
    "\n",
    "augment = bronze_df.withColumn(\"HashKey\", F.sha2(F.concat_ws(\"||\", *bronze_df.columns), 256))\n",
    "\n",
    "convert_to_timestamp = augment \\\n",
    "    .withColumn(\"ImplementedDate\", F.unix_timestamp(\"ImplementedDate\", \"d/MM/yyyy HH:mm\").cast(TimestampType())) \\\n",
    "    .withColumn(\"RequestDate\", F.to_timestamp(\"RequestDate\", 'dd/MM/yyyy HH:mm').cast(TimestampType())) \\\n",
    "    .withColumn(\"LastUpdatedDate\", F.to_timestamp(\"LastUpdatedDate\", 'dd/MM/yyyy HH:mm').cast(TimestampType()))\n",
    "\n",
    "\n",
    "calculate_response = convert_to_timestamp.withColumn(\"Response\", F.datediff(F.col(\"RequestDate\"), F.col(\"ImplementedDate\")))\n",
    "calculate_response.createOrReplaceTempView(\"fastest_response\")\n",
    "\n",
    "fastest_response_query = \"\"\"\n",
    "                        SELECT * \n",
    "                        FROM fastest_response \n",
    "                        ORDER BY Response DESC\n",
    "                        \"\"\"\n",
    "fastest_response_df = spark.sql(fastest_response_query)\n",
    "\n",
    "drop_nulls = fastest_response_df.na.drop(subset=[\"AccountID\"])\n",
    "filter_out = drop_nulls.filter(~drop_nulls.Fibre.startswith('2.67E'))\n",
    "filter_out.repartition(1).write.csv(OUTPUT_PATH + 'silver', sep='|', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.read.csv(OUTPUT_PATH + 'silver/*.csv', sep='|', header=True, schema=gold_schema, enforceSchema=True)\n",
    "silver_df.repartition(1).write.option(\"maxRecordsPerFile\", 1000).json(OUTPUT_PATH + 'gold', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}