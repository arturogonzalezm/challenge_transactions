{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ETL Pipelines ##\n",
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call RAW Transaction.csv file, as well as output file path."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "RAW_FILE_PATH = '../../data/raw/Transaction.csv'\n",
    "OUTPUT_PATH = '../../data/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run schema definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../../schema_definition/schema.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import schema definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from schema_definition.schema import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import Pyspark related libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.shell import spark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a user defined function to add timestamp to ouput folder names."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/16 01:02:27 WARN SimpleFunctionRegistry: The function file_processed_date replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<function __main__.file_processed_date()>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def file_processed_date():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "spark.udf.register(\"file_processed_date\", file_processed_date, TimestampType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This variable enables legacy TimeStamp datatype conversion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[key: string, value: string]"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BRONZE ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This paragraph will read/extract the data from the Transaction.csv file, force the schema to the default 17 columns and write onto a\n",
    "tab separated csv file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/16 01:18:02 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 19, schema size: 17\n",
      "CSV file: file:///Users/arturogonzalez/DataspellProjects/transactions_notebooks/data/raw/Transaction.csv\n"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read.csv(RAW_FILE_PATH, sep=',', header=True, schema=bronze_schema, enforceSchema=True)\n",
    "raw_df.write.format(\"parquet\").mode(\"overwrite\").save(OUTPUT_PATH + 'bronze/' + 'date' + '=' + file_processed_date())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SILVER ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This paragraph will:\n",
    "- Augment the data with a hash key.\n",
    "- Filter out the questionable data.\n",
    "- Apply DataTypes.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_df = spark.read.parquet(OUTPUT_PATH + \"bronze\", mergeSchema=True)\n",
    "\n",
    "augment = bronze_df.withColumn(\"HashKey\", F.sha2(F.concat_ws(\"||\", *bronze_df.columns), 256))\n",
    "\n",
    "convert_to_timestamp = augment \\\n",
    "    .withColumn(\"AccountID\",F.col(\"AccountID\").cast(IntegerType())) \\\n",
    "    .withColumn(\"CODE\",F.col(\"CODE\").cast(IntegerType())) \\\n",
    "    .withColumn(\"ActiveIndicator\",F.col(\"ActiveIndicator\").cast(IntegerType())) \\\n",
    "    .withColumn(\"ImplementedDate\", F.unix_timestamp(\"ImplementedDate\", \"d/MM/yyyy HH:mm\").cast(TimestampType())) \\\n",
    "    .withColumn(\"RequestDate\", F.to_timestamp(\"RequestDate\", 'dd/MM/yyyy HH:mm').cast(TimestampType())) \\\n",
    "    .withColumn(\"StatusCode\",F.col(\"StatusCode\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Amount\",F.col(\"Amount\").cast(DoubleType())) \\\n",
    "    .withColumn(\"AgentID\",F.col(\"AgentID\").cast(IntegerType())) \\\n",
    "    .withColumn(\"LastUpdatedDate\", F.to_timestamp(\"LastUpdatedDate\", 'dd/MM/yyyy HH:mm').cast(TimestampType())) \\\n",
    "    .withColumn(\"PostCode\",F.col(\"PostCode\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "calculate_response = convert_to_timestamp.withColumn(\"Response\", F.datediff(F.col(\"RequestDate\"), F.col(\"ImplementedDate\")))\n",
    "calculate_response.createOrReplaceTempView(\"fastest_response\")\n",
    "\n",
    "fastest_response_query = \"\"\"\n",
    "                        SELECT * \n",
    "                        FROM fastest_response \n",
    "                        ORDER BY Response DESC\n",
    "                        \"\"\"\n",
    "fastest_response_df = spark.sql(fastest_response_query)\n",
    "\n",
    "drop_nulls = fastest_response_df.na.drop(subset=[\"AccountID\"])\n",
    "filter_out = drop_nulls.filter(~drop_nulls.Fibre.startswith('2.67E'))\n",
    "drop_date_column  = filter_out.drop('date')\n",
    "drop_date_column.write.format(\"parquet\").mode(\"overwrite\").save(OUTPUT_PATH + 'silver/' + 'date' + '=' + file_processed_date())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GOLD ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This paragraph will:\n",
    "\n",
    "- When the number of events reach 1000, output the events to a JSON file.\n",
    "- The output filenames should have a batch number e.g. the second 1000 records will go into a file\n",
    "called 2.json and so on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountID: integer (nullable = true)\n",
      " |-- CODE: integer (nullable = true)\n",
      " |-- ImplementedDate: timestamp (nullable = true)\n",
      " |-- ActiveIndicator: integer (nullable = true)\n",
      " |-- AccountType: string (nullable = true)\n",
      " |-- Service: string (nullable = true)\n",
      " |-- BU: string (nullable = true)\n",
      " |-- RequestDate: timestamp (nullable = true)\n",
      " |-- AccountStatus: string (nullable = true)\n",
      " |-- StatusCode: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Version: string (nullable = true)\n",
      " |-- AgentID: integer (nullable = true)\n",
      " |-- Fibre: string (nullable = true)\n",
      " |-- LastUpdatedDate: timestamp (nullable = true)\n",
      " |-- PropertyType: string (nullable = true)\n",
      " |-- PostCode: integer (nullable = true)\n",
      " |-- HashKey: string (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_df = spark.read.parquet(OUTPUT_PATH + \"silver\", mergeSchema=True)\n",
    "drop_date_column  = silver_df.drop('date')\n",
    "drop_date_column.coalesce(1).write.option(\"maxRecordsPerFile\", 1000).json(OUTPUT_PATH + 'gold', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}